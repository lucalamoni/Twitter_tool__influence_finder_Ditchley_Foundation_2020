{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the graph database and production of statistics\n",
    "\n",
    "In this notebook we will create a graph database from twitter data and run ranking and clustering algorithms. The notebook is in python3, but this is just a wrapper for cypher, the Neo4j language. \n",
    "\n",
    "## Install and configure Neo4j\n",
    "\n",
    "Before running this notebook one needs to have the Neo4j desktop development environment up and running. The Neo4j development app can be downloaded from here:\n",
    "\n",
    "https://neo4j.com/download/?ref=try-neo4j-lp\n",
    "\n",
    "There is also and online \"sandbox\" version, this is actually quite useful because it contains an example of twitter analytics. Neo4j can also be run from a cypher terminal, which comes with the desktop installation. We need to install the APOC and Graph Data Science Library plugins. These can be done via the desktop app. \n",
    "\n",
    "On the desktop, create a new database by clicking \"add database\" and selecting \"create local database\". We shall name our databse \"Cyber Journalists\" and give it the password \"tweetoftheday\". Then click \"create\", then \"start\", a new window will open.\n",
    "\n",
    "If your installation is anything like mine this will create a database in an obscure location on your computer. On my machine it has this address:\n",
    "\n",
    "/Users/adam/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-31caf974-2ca6-4ba8-96ad-0adde23efd92/installation-4.1.0\n",
    "\n",
    "For convenience the first thing we'll do is move into this directory. If anyone can figure out how to tell it to put the database in a sensible location please let me know :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adam/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-31caf974-2ca6-4ba8-96ad-0adde23efd92/installation-4.1.0\n"
     ]
    }
   ],
   "source": [
    "%cd \"/Users/adam/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-31caf974-2ca6-4ba8-96ad-0adde23efd92/installation-4.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we will want to be able to read in and write to different file formats. To do this we need to locate the configuration file and add the following couple of lines to location/conf/neo4j.conf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Enable loading of json files                                                                                                                                                                  \n",
    "apoc.import.file.enabled=true\n",
    "\n",
    "# Enable exportation of files                                                                                                                                                                   \n",
    "apoc.export.file.enabled=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to install py2neo in order to wrap cypher commands inside a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py2neo in /Users/adam/anaconda3/lib/python3.6/site-packages (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/Users/adam/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install py2neo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, pretty much everything can be done with this package, but I did not find the syntax to be particularly useful. Instead we will give Neo4j commands in cypher, the graph database language. The first thing we need to do is to get python to log into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Transaction at 0x11519a978>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "# load / declare the database\n",
    "graph = Graph(\"bolt://localhost:7687\", user=\"neo4j\", password=\"tweetoftheday\")\n",
    "graph.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty graph, obviously don't run this if you already have stuff in there you don't want to delete\n",
    "graph.delete_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start giving cypher commands, in this notebook will will write them as strings and then pass them to neo4j using py2neo. We could also run them in the desktop or the cypher shell. \n",
    "\n",
    "First we want to load in our tweet information, to do this we need to put the file containing the tweets in the location/import directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: /Users/adam/Downloads/standardised_cyber_tweets.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mv ~/Downloads/standardised_cyber_tweets.csv import/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create nodes representing tweets and users\n",
    "\n",
    "Below, we use three cypher commands. The first loads the file. The second creates nodes representing tweets. The third creates nodes representing people. Note that \"CREATE\" and \"MERGE\" are slightly different. \"CREATE\" makes a new node but if that node already exists then it does nothing. \"MERGE\" creates a node if it doesn't already exist, and if it does exist will add or update information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x1151950b8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in tweets and twitter user information\n",
    "query_string = '''\n",
    "   LOAD CSV WITH HEADERS FROM \"file:///standardised_cyber_tweets.csv\" AS row\n",
    "   \n",
    "   CREATE (t:Tweet {tweet_id: row.tweet_id, conversation_id: row.conversation_id, user_id: row.user_id, \n",
    "   reply_to: row.reply_to, tweet_created_at_date: row.tweet_created_at_date, \n",
    "   tweet_created_at_time: row.tweet_created_at_time, text: row.text, replies_count: row.replies_count, \n",
    "   retweets_count: row.reteets_count, favourite_count: row.favourite_count, likes_count: row.likes_count,\n",
    "   hashtags: row.hashtags, topics: row.topics})\n",
    "   \n",
    "   MERGE (p:Person {user_id: row.user_id, screen_name: row.screen_name, name: row.name, \n",
    "   user_description: row.user_description, user_friends_n: row.user_friends_n, user_followers_n: row.user_followers_n, \n",
    "   prof_created_at: row.prof_created_at, favourites_count: row.favourites_count, verified: row.verified, \n",
    "   statuses_count: row.statuses_count});\n",
    "   '''\n",
    "# run cypher query\n",
    "graph.run(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw edges \n",
    "\n",
    "Now we will use \"MATCH\" to find who tweeted what and draw edges between people and tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x115195550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = '''// Create edges linking tweeters and tweets\n",
    "            MATCH (t:Tweet), (p:Person)\n",
    "            WHERE t.user_id = p.user_id\n",
    "            MERGE (p)-[:POSTS]->(t)'''\n",
    "graph.run(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load follower information\n",
    "\n",
    "Move the friends file to the import directory. This file should be a csv file containing two columns with headings:\n",
    "\n",
    "screen_name,friend\n",
    "\n",
    "Depending on your machine this file may be too big to load in all at once. This can be avoided by splitting the file up into smaller files. Just make sure you put the headings at the top of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x115195898>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = '''// match link between users and friends in the database, if friend doesnt exisit then create it\n",
    "            LOAD CSV WITH HEADERS FROM \"file:///cyber_journalist_friends_2.csv\" AS row\n",
    "            MATCH (p:Person) WHERE p.screen_name = row.screen_name\n",
    "            MERGE (n:Person {screen_name: row.friend})\n",
    "            MERGE (p)-[:FOLLOWS]->(n)'''\n",
    "graph.run(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load mentions\n",
    "\n",
    "Move the mentions file to the import directory. This file should be a csv file containing two columns with headings:\n",
    "\n",
    "tweet_is,mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x1151985c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = '''// match link between tweets and mentions\n",
    "            LOAD CSV WITH HEADERS FROM \"file:///tst.csv\" AS row\n",
    "            MATCH (t:Tweet) WHERE t.tweet_id = row.tweet_id\n",
    "            MERGE (p:Person {screen_name: row.mentions})\n",
    "            MERGE (t)-[:MENTIONS]->(p)'''\n",
    "graph.run(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to start running our network analysis algorithms. If you'd like to visualize the graph then please scroll down to the graph visualization section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality and clustering\n",
    "\n",
    "In this section we will run the Page rank algorithm to measure the centrality of users and the Lauvain algorithm to look for communities within the graph. These algorithms are part of the Graph Data Science plugin that we installed earlier. If the plugins are not installed for this graph then you will need to install them and restart the graph.\n",
    "\n",
    "To start with we must create a \"named graph\", this lists the components of the graph that we want to consider when running our algorithms. We'll start by just looking at the followers but this can be extended later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x11519a860>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = '''// to run gds algorithms one needs to create a named graph\n",
    "                CALL gds.graph.create(\n",
    "                'my-native-graph',\n",
    "                'Person',\n",
    "                'FOLLOWS'\n",
    "                )\n",
    "                YIELD graphName, nodeCount, relationshipCount, createMillis'''\n",
    "graph.run(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page rank\n",
    "\n",
    "All the graph algorithms in GDS have the same syntax(ish) which makes them easy to use. \n",
    "\n",
    "By running Page rank on the Person nodes linked just by the FOLLOWS edges we rank the journalists' friends, ordered by the number of journalists who follow them, but weighted by the number of followers each journalist has amongst the ensemble of journalists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''// run pagerank and return the ten highest scoring accounts\n",
    "CALL gds.pageRank.stream('my-native-graph') \n",
    "YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).screen_name AS screen_name, score\n",
    "ORDER BY score DESC, screen_name ASC LIMIT 10'''\n",
    "ans = graph.run(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to do some python analysis on these results so let's put them in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       screen name      rank\n",
      "0      jimwaterson  0.150876\n",
      "1  MalwareTechBlog  0.150768\n",
      "2     ProfWoodward  0.150723\n",
      "3        ruskin147  0.150723\n",
      "4    openDemocracy  0.150684\n",
      "5       DanRaywood  0.150676\n",
      "6        DaveLeeFT  0.150623\n",
      "7       ForbesTech  0.150623\n",
      "8        LeoKelion  0.150623\n",
      "9      Scott_Helme  0.150623\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(ans, columns=['screen name', 'rank'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to look for people we don't yet have the full information for we can slightly modify this code to exclude users already in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''// run pagerank and get the 100 highest scoring accounts without user information in the database\n",
    "                CALL gds.pageRank.stream('my-native-graph') \n",
    "                YIELD nodeId, score\n",
    "                WHERE NOT EXISTS(gds.util.asNode(nodeId).name)\n",
    "                RETURN gds.util.asNode(nodeId).screen_name AS screen_name, score, gds.util.asNode(nodeId).name AS name\n",
    "                ORDER BY score DESC, screen_name ASC LIMIT 100'''\n",
    "ans = graph.run(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Louvain communities\n",
    "\n",
    "The Louvain method is an algorithm to detect communities in large networks. It maximizes a modularity score for each community, where the modularity quantifies the quality of an assignment of nodes to communities. This means evaluating how much more densely connected the nodes within a community are, compared to how connected they would be in a random network.\n",
    "\n",
    "The Louvain algorithm is a hierarchical clustering algorithm, that recursively merges communities into a single node and executes the modularity clustering on the condensed graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name  communityId                  intermediateCommunityIds\n",
      "0       leokelion            2            [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "1    sophiafurber            7            [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "2    scfgallagher            8            [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "3  mshannahmurphy            9            [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "4   jesscahaworth           10  [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "5    ad_nauseum74           11  [11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "6   DaveColePhoto           24  [21, 21, 21, 21, 21, 21, 21, 21, 23, 24]\n",
      "7      EliseKapNM           24  [15, 15, 15, 15, 16, 18, 20, 21, 23, 24]\n",
      "8  HashemOsseiran           24  [13, 13, 14, 15, 16, 18, 20, 21, 23, 24]\n",
      "9    RobaHusseini           24  [12, 13, 14, 15, 16, 18, 20, 21, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "query_string = '''// run Louvain algorithm to identify communities, return 10\n",
    "CALL gds.louvain.stream('my-native-graph', {includeIntermediateCommunities: true}) \n",
    "YIELD nodeId, communityId, intermediateCommunityIds\n",
    "RETURN gds.util.asNode(nodeId).screen_name AS screen_name, communityId, intermediateCommunityIds\n",
    "ORDER BY communityId ASC, screen_name ASC LIMIT 10'''\n",
    "ans = graph.run(query_string)\n",
    "df = pd.DataFrame.from_records(ans, columns=['screen_name', 'communityId','intermediateCommunityIds'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the groups by the number of members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   communityId  number of members\n",
      "0         2856                 11\n",
      "1          100                 11\n",
      "2           24                 11\n",
      "3          214                 11\n",
      "4         2904                 11\n",
      "5          192                 11\n",
      "6            2                  1\n",
      "7            9                  1\n",
      "8            8                  1\n",
      "9            7                  1\n"
     ]
    }
   ],
   "source": [
    "query_string = '''// count how many members each community has\n",
    "CALL gds.louvain.stream('my-native-graph')\n",
    "YIELD nodeId, communityId\n",
    "RETURN communityId, COUNT(DISTINCT nodeId) AS members\n",
    "ORDER BY members DESC LIMIT 10'''\n",
    "ans = graph.run(query_string)\n",
    "\n",
    "df = pd.DataFrame.from_records(ans, columns=['communityId', 'number of members'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other bits and bobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering tweets\n",
    "\n",
    "Depending on the source of data (the Twitter API or twint) tweets either contain a \"conversation_id\" or a \"reply_to_status_id\". These are different but related numbers. The \"conversation_id\" is the id of the first tweet in the conversation, the \"reply_to_status_id\" is the id of the tweet in the conversation preceding the current tweet. Tweets also contain timestamp information, thus even if we don't have the \"reply_to_status_id\" we can order tweets in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by finding tweets in the same conversation by matching conversation_id\n",
    "query_string = '''// Find tweets in the same conversation\n",
    "            MATCH (t1:Tweet), (t2:Tweet)\n",
    "            WHERE t1.conversation_id = t2.conversation_id\n",
    "            AND t1.tweet_id <> t2.tweet_id\n",
    "            AND t1.tweet_created_at_date <= t2.tweet_created_at_date\n",
    "            AND t1.tweet_created_at_time < t2.tweet_created_at_time\n",
    "            MERGE (t1)-[:CONVERSATION]->(t2)'''\n",
    "graph.run(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversations should be a chain rather than a mesh, although very occasionally tweets may have the same time stamp\n",
    "query_string = '''// Find the direction of the conversation\n",
    "            MATCH (t1)-[c1:CONVERSATION]->(t2), (t1)-[c2:CONVERSATION]->(t3) \n",
    "            WHERE t2.tweet_created_at_date < t3.tweet_created_at_date\n",
    "            DELETE c2;'''\n",
    "graph.run(query_string)\n",
    "\n",
    "query_string = '''MATCH (t1)-[c1:CONVERSATION]->(t2), (t1)-[c2:CONVERSATION]->(t3) \n",
    "            WHERE t2.tweet_created_at_time < t3.tweet_created_at_time\n",
    "            DELETE c2;'''\n",
    "graph.run(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted interactions\n",
    "\n",
    "As discussed, we may want to develop our ranking algorithm using, for example, weighted edges. Here we will add edges representing interactions between users weighted by the number of times they are in the same conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''// make edges between journalists, weighted by the number of interactions\n",
    "            MATCH path=(t1)-[:CONVERSATION]-(t2)\n",
    "            WHERE t1.user_id <> t2.user_id\n",
    "            MATCH (p1:Person), (p2:Person)\n",
    "            WHERE p1.user_id = t1.user_id AND p2.user_id = t2.user_id\n",
    "            WITH p1,p2, COUNT(path) AS weight\n",
    "            MERGE (p1)-[i:INTERACTION]-(p2)\n",
    "            SET i.strength = weight'''\n",
    "graph.run(query_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
